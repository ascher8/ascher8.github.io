<!DOCTYPE html>
<html>
<head>
  <title>Aaron Scher Personal Website</title>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
</head>
<body>

  <!-- Header -->
  <header>
    <h1>Welcome to My Website!</h1>
</header>

<!-- Introduction Section -->
<section>
    <p>
    I'm Aaron Scher, an independent researcher working on AI safety. Some of my main focuses are sycophancy, chain-of-thought faithfulness, and interpretability in large language models. 
    I'm broadly interested in empirical research with current AI systems that can inform the safe design and deployment of future, more powerful AIs.
    </p>
    <p>
    My background is in psychology and economics, and in early 2022 I started learning about AI and AI safety. 
    In summer 2023 I started doing independent AI alignment research with the <a href="https://www.matsprogram.org/">MATS program. </a>
    In fall 2023 I've been continuing independent research, funded by a grant from <a href="https://www.openphilanthropy.org/">Open Philanthropy. </a>I've also been managing 4 research teams through <a href="https://bit.ly/sparai">SPAR</a>, the Supervised Program in Alignment Research. 
    </p>
    <p>
        
    </p>
</section>

<!-- Blog Section -->
<section>
    <h2>I sometimes write about AI safety and other topics I'm interested in</h2>
    <p>I blog in a few places: </p>
        <ul>
            <li><a href="https://www.lesswrong.com/users/aaron_scher">LessWrong</a></li>
            <li><a href="https://forum.effectivealtruism.org/users/aaron_scher">EA Forum</a></li>
            <li><a href="https://aaronscher.substack.com/">Substack</a></li>
        </ul>
   
    <p>Here are a couple posts I'm particularly happy with: </p>
        <ul>
            <li><a href="https://aaronscher.substack.com/p/consolidation-mechanisms-for-agi">Consolidation mechanisms for AGI labs should be connected to AI capabilities
            </a></li>
            <li><a href="https://forum.effectivealtruism.org/posts/bLWG7onTMKzdozez8/it-s-not-obvious-that-getting-dangerous-ai-later-is-better">Itâ€™s not obvious that getting dangerous AI later is better</a></li>
        </ul>
 
</section>

<!-- Links Section -->
<section>
    <h2>Links</h2>

        <ul>
            <li><a href="https://www.linkedin.com/in/aaron-scher-303b0a19a/">LinkedIn</a></li>
            <li><a href="https://twitter.com/AvailableName8">Twitter - mostly lurking</a></li>
            <li><a href="https://github.com/ascher8">Github</a></li>
            <li>
                <script type="text/javascript">
                    var username = "aaronscher8";
                    var hostname = "gmail.com";
                    var linktext = username + "@" + hostname ;
                    document.write("<a href='mailto:" + username + "@" + hostname + "'>" + linktext + "</a>");
                </script>
                <noscript>
                    Email: [at] gmail [dot] com
                </noscript>
            </li>
        </ul>

</section>

<!-- Footer -->
<footer>
    <p>Last updated November 2023</p>
</footer>

</body>
</html>